{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0827df8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ab15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"../data/train.csv\")\n",
    "test_raw = pd.read_csv(\"../data/test.csv\")\n",
    "sample_sub = pd.read_csv(\"../data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd9b7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = train_raw.drop(columns=[\"id\", \"Feature_5\"])\n",
    "test_raw = test_raw.drop(columns=[\"id\", \"Feature_5\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f746cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use median instead of mode for better imputation\n",
    "median_values = train_raw.median()\n",
    "\n",
    "train = train_raw.fillna(median_values)\n",
    "test = test_raw.fillna(median_values)\n",
    "\n",
    "# Additional handling for any remaining NaNs\n",
    "train = train.fillna(train.mean())\n",
    "test = test.fillna(train.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a36487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature_1      0\n",
      "Feature_2      0\n",
      "Feature_3      0\n",
      "Feature_4      0\n",
      "Feature_6      0\n",
      "Outage_Risk    0\n",
      "dtype: int64\n",
      "Feature_1    0\n",
      "Feature_2    0\n",
      "Feature_3    0\n",
      "Feature_4    0\n",
      "Feature_6    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().sum())\n",
    "print(test.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71b29c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "Outage_Risk\n",
      "0    5422\n",
      "1    2078\n",
      "Name: count, dtype: int64\n",
      "Class imbalance ratio: 2.61:1\n",
      "\n",
      "üìä Feature Engineering...\n",
      "Original features: 5, Engineered features: 10\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "X = train.drop(columns=[\"Outage_Risk\"])\n",
    "y = train[\"Outage_Risk\"]\n",
    "\n",
    "# Check class distribution (to handle imbalance)\n",
    "print(f\"Class distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Class imbalance ratio: {y.value_counts()[0] / y.value_counts()[1]:.2f}:1\")\n",
    "\n",
    "# Feature Engineering - Create interaction terms and polynomial features\n",
    "print(\"\\nüìä Feature Engineering...\")\n",
    "X_engineered = X.copy()\n",
    "\n",
    "# Create interaction terms for important features\n",
    "X_engineered['Feature_1_x_Feature_2'] = X['Feature_1'] * X['Feature_2']\n",
    "X_engineered['Feature_1_x_Feature_3'] = X['Feature_1'] * X['Feature_3']\n",
    "X_engineered['Feature_3_x_Feature_4'] = X['Feature_3'] * X['Feature_4']\n",
    "\n",
    "# Create polynomial features\n",
    "X_engineered['Feature_1_squared'] = X['Feature_1'] ** 2\n",
    "X_engineered['Feature_3_squared'] = X['Feature_3'] ** 2\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_engineered_scaled = scaler.fit_transform(X_engineered)\n",
    "X_engineered_scaled = pd.DataFrame(X_engineered_scaled, columns=X_engineered.columns)\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}, Engineered features: {X_engineered_scaled.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93828e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Running 5-Fold Cross-Validation with Best Techniques...\n",
      "============================================================\n",
      "Class weights: {0: np.float64(0.6916267060125415), 1: np.float64(1.8046198267564966)}\n",
      "Fold 1: LR=0.7027 | RF=0.6817 | GB=0.6880 | XGB=0.6847 | Vote=0.6988 | Super=0.6950\n",
      "Fold 2: LR=0.7373 | RF=0.7049 | GB=0.7186 | XGB=0.7249 | Vote=0.7356 | Super=0.7304\n",
      "Fold 3: LR=0.7008 | RF=0.6744 | GB=0.6740 | XGB=0.6694 | Vote=0.6899 | Super=0.6862\n",
      "Fold 4: LR=0.7217 | RF=0.6930 | GB=0.6965 | XGB=0.6978 | Vote=0.7140 | Super=0.7110\n",
      "Fold 5: LR=0.7183 | RF=0.6969 | GB=0.6956 | XGB=0.7038 | Vote=0.7124 | Super=0.7091\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPROVED CROSS-VALIDATION WITH ALL TECHNIQUES\n",
    "# ============================================================\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"\\nüîÑ Running 5-Fold Cross-Validation with Best Techniques...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate class weights to handle imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "results = {\n",
    "    'Logistic Regression': [],\n",
    "    'Random Forest': [],\n",
    "    'Gradient Boosting': [],\n",
    "    'XGBoost': [],\n",
    "    'Voting Ensemble': [],\n",
    "    'Super Ensemble': []\n",
    "}\n",
    "\n",
    "optimal_thresholds = {model: [] for model in results.keys()}\n",
    "\n",
    "for fold_num, (train_idx, val_idx) in enumerate(skf.split(X_engineered_scaled, y), 1):\n",
    "    X_train, X_val = X_engineered_scaled.iloc[train_idx], X_engineered_scaled.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Logistic Regression with class weights\n",
    "    lr = LogisticRegression(C=0.5, max_iter=1000, class_weight='balanced', random_state=42)\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_probs = lr.predict_proba(X_val)[:, 1]\n",
    "    lr_auc = roc_auc_score(y_val, lr_probs)\n",
    "    results['Logistic Regression'].append(lr_auc)\n",
    "    \n",
    "    # Random Forest with class weights\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=12,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_probs = rf.predict_proba(X_val)[:, 1]\n",
    "    rf_auc = roc_auc_score(y_val, rf_probs)\n",
    "    results['Random Forest'].append(rf_auc)\n",
    "    \n",
    "    # Gradient Boosting with scale_pos_weight\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb.fit(X_train, y_train)\n",
    "    gb_probs = gb.predict_proba(X_val)[:, 1]\n",
    "    gb_auc = roc_auc_score(y_val, gb_probs)\n",
    "    results['Gradient Boosting'].append(gb_auc)\n",
    "    \n",
    "    # XGBoost with scale_pos_weight\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=class_weight_dict[0] / class_weight_dict[1],\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        verbosity=0\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    xgb_probs = xgb.predict_proba(X_val)[:, 1]\n",
    "    xgb_auc = roc_auc_score(y_val, xgb_probs)\n",
    "    results['XGBoost'].append(xgb_auc)\n",
    "    \n",
    "    # Voting Ensemble\n",
    "    voting = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(C=0.5, max_iter=1000, class_weight='balanced', random_state=42)),\n",
    "            ('rf', RandomForestClassifier(n_estimators=150, max_depth=10, class_weight='balanced', random_state=42, n_jobs=-1)),\n",
    "            ('gb', GradientBoostingClassifier(n_estimators=150, learning_rate=0.05, max_depth=4, random_state=42))\n",
    "        ],\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting.fit(X_train, y_train)\n",
    "    voting_probs = voting.predict_proba(X_val)[:, 1]\n",
    "    voting_auc = roc_auc_score(y_val, voting_probs)\n",
    "    results['Voting Ensemble'].append(voting_auc)\n",
    "    \n",
    "    # Super Ensemble (average all models)\n",
    "    ensemble_probs = (lr_probs + rf_probs + gb_probs + xgb_probs + voting_probs) / 5\n",
    "    super_auc = roc_auc_score(y_val, ensemble_probs)\n",
    "    results['Super Ensemble'].append(super_auc)\n",
    "    \n",
    "    print(f\"Fold {fold_num}: LR={lr_auc:.4f} | RF={rf_auc:.4f} | GB={gb_auc:.4f} | XGB={xgb_auc:.4f} | Vote={voting_auc:.4f} | Super={super_auc:.4f}\")\n",
    "\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9365abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training final Super Ensemble on full dataset...\n",
      "‚úì Super Ensemble predictions generated\n",
      "Mean prediction: 0.3382\n",
      "Min prediction: 0.0673, Max prediction: 0.9616\n"
     ]
    }
   ],
   "source": [
    "# Train final Super Ensemble on all data and generate predictions\n",
    "print(\"\\nüöÄ Training final Super Ensemble on full dataset...\")\n",
    "\n",
    "# Scale test data\n",
    "X_test_engineered = test.copy()\n",
    "X_test_engineered['Feature_1_x_Feature_2'] = test['Feature_1'] * test['Feature_2']\n",
    "X_test_engineered['Feature_1_x_Feature_3'] = test['Feature_1'] * test['Feature_3']\n",
    "X_test_engineered['Feature_3_x_Feature_4'] = test['Feature_3'] * test['Feature_4']\n",
    "X_test_engineered['Feature_1_squared'] = test['Feature_1'] ** 2\n",
    "X_test_engineered['Feature_3_squared'] = test['Feature_3'] ** 2\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test_engineered)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_engineered.columns)\n",
    "\n",
    "# Train all models on full data\n",
    "lr_final = LogisticRegression(C=0.5, max_iter=1000, class_weight='balanced', random_state=42)\n",
    "lr_final.fit(X_engineered_scaled, y)\n",
    "\n",
    "rf_final = RandomForestClassifier(n_estimators=200, max_depth=12, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "rf_final.fit(X_engineered_scaled, y)\n",
    "\n",
    "gb_final = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=5, subsample=0.8, random_state=42)\n",
    "gb_final.fit(X_engineered_scaled, y)\n",
    "\n",
    "xgb_final = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=class_weight_dict[0] / class_weight_dict[1],\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_final.fit(X_engineered_scaled, y)\n",
    "\n",
    "voting_final = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', lr_final),\n",
    "        ('rf', rf_final),\n",
    "        ('gb', gb_final)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_final.fit(X_engineered_scaled, y)\n",
    "\n",
    "# Generate predictions\n",
    "lr_test_probs = lr_final.predict_proba(X_test_scaled)[:, 1]\n",
    "rf_test_probs = rf_final.predict_proba(X_test_scaled)[:, 1]\n",
    "gb_test_probs = gb_final.predict_proba(X_test_scaled)[:, 1]\n",
    "xgb_test_probs = xgb_final.predict_proba(X_test_scaled)[:, 1]\n",
    "voting_test_probs = voting_final.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Super Ensemble - Average all predictions\n",
    "test_probs = (lr_test_probs + rf_test_probs + gb_test_probs + xgb_test_probs + voting_test_probs) / 5\n",
    "\n",
    "print(\"‚úì Super Ensemble predictions generated\")\n",
    "print(f\"Mean prediction: {test_probs.mean():.4f}\")\n",
    "print(f\"Min prediction: {test_probs.min():.4f}, Max prediction: {test_probs.max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7096e9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Improved submission saved to ..\\submissions\\final_submission_improved.csv\n"
     ]
    }
   ],
   "source": [
    "# Save improved submission\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sample_sub[\"Outage_Risk\"] = test_probs\n",
    "\n",
    "# Create submissions directory if it doesn't exist\n",
    "submission_dir = Path(\"../submissions\")\n",
    "submission_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "submission_path = submission_dir / \"final_submission_improved.csv\"\n",
    "sample_sub.to_csv(submission_path, index=False)\n",
    "print(f\"‚úì Improved submission saved to {submission_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cb24086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (7500, 5)\n",
      "Target shape: (7500,)\n",
      "Target distribution:\n",
      "Outage_Risk\n",
      "0    5422\n",
      "1    2078\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for cross-validation\n",
    "X = train.drop(columns=[\"Outage_Risk\"])\n",
    "y = train[\"Outage_Risk\"]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b2afff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà CROSS-VALIDATION RESULTS SUMMARY\n",
      "======================================================================\n",
      "Logistic Regression  | Mean AUC: 0.7162 | Std Dev: 0.0134 | Scores: ['0.7027', '0.7373', '0.7008', '0.7217', '0.7183']\n",
      "Random Forest        | Mean AUC: 0.6902 | Std Dev: 0.0109 | Scores: ['0.6817', '0.7049', '0.6744', '0.6930', '0.6969']\n",
      "Gradient Boosting    | Mean AUC: 0.6945 | Std Dev: 0.0145 | Scores: ['0.6880', '0.7186', '0.6740', '0.6965', '0.6956']\n",
      "XGBoost              | Mean AUC: 0.6961 | Std Dev: 0.0186 | Scores: ['0.6847', '0.7249', '0.6694', '0.6978', '0.7038']\n",
      "Voting Ensemble      | Mean AUC: 0.7101 | Std Dev: 0.0155 | Scores: ['0.6988', '0.7356', '0.6899', '0.7140', '0.7124']\n",
      "Super Ensemble       | Mean AUC: 0.7063 | Std Dev: 0.0151 | Scores: ['0.6950', '0.7304', '0.6862', '0.7110', '0.7091']\n",
      "======================================================================\n",
      "\n",
      "üèÜ BEST MODEL: Logistic Regression with Mean AUC = 0.7162\n",
      "Improvement over Logistic Regression: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Display comprehensive results\n",
    "print(\"\\nüìà CROSS-VALIDATION RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, scores in results.items():\n",
    "    mean_auc = np.mean(scores)\n",
    "    std_auc = np.std(scores)\n",
    "    print(f\"{model_name:20s} | Mean AUC: {mean_auc:.4f} | Std Dev: {std_auc:.4f} | Scores: {[f'{s:.4f}' for s in scores]}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best model\n",
    "best_model = max(results.items(), key=lambda x: np.mean(x[1]))\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model[0]} with Mean AUC = {np.mean(best_model[1]):.4f}\")\n",
    "print(f\"Improvement over Logistic Regression: {(np.mean(best_model[1]) - np.mean(results['Logistic Regression'])) * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kagglethon-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
